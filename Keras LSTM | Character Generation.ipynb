{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### I have a GOT dataset for season 1 and 2\n",
    "Firstly lets preprocess it, before feeding to our Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000193 80\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "data = open(\"datasets/got.txt\",\"r\",encoding=\"utf-8\").read()\n",
    "chars = pickle.load(open(\"Pickles and model Weights/chars.pickle\",\"rb\")) #total unique characters\n",
    "#chars = list(set(data)) \n",
    "VOCAB_SIZE = len(chars)\n",
    "print(len(data),VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Mapping\n",
    "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
    "char_to_idx = {char: i for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "number_of_features = VOCAL_SIZE\n",
    "length_of_sequence = how many chars, model will look at a time\n",
    "number_of_sequence = len(data)/length_of_sequence\n",
    "\"\"\"\n",
    "SEQ_LENGTH = 60 #input sequence length\n",
    "N_FEATURES = VOCAB_SIZE #one hot encoding here, that's why, but deduplicated for clarity\n",
    "\n",
    "N_SEQ = int(np.floor((len(data) - 1) / SEQ_LENGTH))\n",
    "\n",
    "X = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
    "y = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(N_SEQ):\n",
    "    X_sequence = data[i * SEQ_LENGTH: (i + 1) * SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_to_idx[c] for c in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1. #one-hot encoding of the input characters\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = data[i * SEQ_LENGTH + 1: (i + 1) * SEQ_LENGTH + 1] #shifted by 1 to the right\n",
    "    y_sequence_ix = [char_to_idx[c] for c in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1. #one-hot encoding of the target characters\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok Now lets create a keras model\n",
    "1. Model is described below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Dense, Activation\n",
    "# constant parameter for the model\n",
    "HIDDEN_DIM = 700 #size of each hidden layer, \"each layer has 700 hidden states\"\n",
    "LAYER_NUM = 2 #number of hidden layers, how much were used?\n",
    "NB_EPOCHS = 50 #max number of epochs to train, \"200 epochs\"\n",
    "BATCH_SIZE = 128 \n",
    "VALIDATION_SPLIT = 0.1 #proportion of the batch used for validation at each epoch\n",
    "\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(HIDDEN_DIM, \n",
    "               input_shape=(None, VOCAB_SIZE), \n",
    "               return_sequences=True))\n",
    "    for _ in range(LAYER_NUM - 1):\n",
    "        model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, length,start_char=None):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [idx_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1.\n",
    "        ix = np.argmax(model.predict(X[:, :i+1,:])[0], 1)\n",
    "        y_char.append(idx_to_char[ix[-1]])\n",
    "    return ''.join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "# callback to save the model if better\n",
    "filepath=\"tgt_model.hdf5\"\n",
    "save_model_cb = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "# callback to stop the training if no improvement\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# callback to generate text at epoch end\n",
    "class generateText(Callback):\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        print(generate_text(self.model, 100))\n",
    "generate_text_cb = generateText()\n",
    "\n",
    "callbacks_list = [save_model_cb]\n",
    "\n",
    "def train(model):\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=2, \n",
    "              epochs=NB_EPOCHS, callbacks=callbacks_list, \n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "    model.save_weights('Pickles and model Weights/text_gen_got1.hdf5')\n",
    "\n",
    "def load_weigths(model):\n",
    "    model.load_weights('Pickles and model Weights/text_gen_got1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel();\n",
    "load_weigths(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But why could she do that? Could here betrayer me, I need you lost him? Why do you help Lord Botce I '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(but the Red Viper of Dothraki were coming across the year of hooves. Not truly, and he was wasted. A scuntle stringy pale played and get his long near of his sword, making pawisions and white linen breechchss smells no hatter. She wished heavy on her sword and the shed women ate the high road.\\n\\nMy o'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model has learned where to use '?' and where to use capital alphabet\n",
    "#lets print some more\n",
    "generate_text(model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the seamstress agreed, as she and Robb when he asked. “Who Sunspean we shall ask fore?”\\n\\n“I wish I had a good heart. Hystera and giants mean to be king. Yet dare I never marry the ship?”\\n\\n“A sword,” Sam agreed, “even the Hand of the King’s Justice,” he said in a low voice, as the flesh beneath the door and retreaked the warhoons strides at darkness, streaming it into his leaves, and he laughed.\\n\\nOr little stubborn to take a knight to let the sky. No one knew we lift the Stormcrows down to the Wall, and there is no weakness lowing at being a quilt.”\\n\\n“You and you will expect Aeso at the Mugitaw’'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#less meaningfull, but atleast most of the words are correct\n",
    "#Lets print a full sequence\n",
    "generate_text(model,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "![Model](images/model_description.png)\n",
    "The input shape of the text data is ordered as follows : (batch size, number of time steps, hidden size). In other words, for each batch sample and each word in the number of time steps, there is a 500 length embedding word vector to represent the input word. These embedding vectors will be learnt as part of the overall model learning. The input data is then fed into two “stacked” layers of LSTM cells (of 500 length hidden size) – in the diagram above, the LSTM network is shown as unrolled over all the time steps. The output from these unrolled cells is still (batch size, number of time steps, hidden size).\n",
    "\n",
    "This output data is then passed to a Keras layer called TimeDistributed, which will be explained more fully below. Finally, the output layer has a softmax activation applied to it. This output is compared to the training y data for each batch, and the error and gradient back propagation is performed from there in Keras. The training y data in this case is the input x words advanced one time step – in other words, at each time step the model is trying to predict the very next word in the sequence. However, it does this at every time step – hence the output layer has the same number of time steps as the input layer. This will be made more clear later.\n",
    "\n",
    "There is a special Keras layer for use in recurrent neural networks called TimeDistributed. This function adds an independent layer for each time step in the recurrent model. So, for instance, if we have 10 time steps in a model, a TimeDistributed layer operating on a Dense layer would produce 10 independent Dense layers, one for each time step. The activation for these dense layers is set to be softmax in the final layer of our Keras LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
