{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### I have a GOT dataset for season 1 and 2\n",
    "Firstly lets preprocess it, before feeding to our Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000193 80\n"
     ]
    }
   ],
   "source": [
    "data = open(\"datasets/got.txt\",\"r\",encoding=\"utf-8\").read()\n",
    "chars = list(set(data)) #total unique characters\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(len(data),VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize Mapping\n",
    "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
    "char_to_idx = {char: i for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "number_of_features = VOCAL_SIZE\n",
    "length_of_sequence = how many chars, model will look at a time\n",
    "number_of_sequence = len(data)/length_of_sequence\n",
    "\"\"\"\n",
    "SEQ_LENGTH = 60 #input sequence length\n",
    "N_FEATURES = VOCAB_SIZE #one hot encoding here, that's why, but deduplicated for clarity\n",
    "\n",
    "N_SEQ = int(np.floor((len(data) - 1) / SEQ_LENGTH))\n",
    "\n",
    "X = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
    "y = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(N_SEQ):\n",
    "    X_sequence = data[i * SEQ_LENGTH: (i + 1) * SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_to_idx[c] for c in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1. #one-hot encoding of the input characters\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = data[i * SEQ_LENGTH + 1: (i + 1) * SEQ_LENGTH + 1] #shifted by 1 to the right\n",
    "    y_sequence_ix = [char_to_idx[c] for c in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1. #one-hot encoding of the target characters\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok Now lets create a keras model\n",
    "1. Model is described below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, TimeDistributed, Dense, Activation\n",
    "# constant parameter for the model\n",
    "HIDDEN_DIM = 700 #size of each hidden layer, \"each layer has 700 hidden states\"\n",
    "LAYER_NUM = 2 #number of hidden layers, how much were used?\n",
    "NB_EPOCHS = 50 #max number of epochs to train, \"200 epochs\"\n",
    "BATCH_SIZE = 128 \n",
    "VALIDATION_SPLIT = 0.1 #proportion of the batch used for validation at each epoch\n",
    "\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(HIDDEN_DIM, \n",
    "               input_shape=(None, VOCAB_SIZE), \n",
    "               return_sequences=True))\n",
    "    for _ in range(LAYER_NUM - 1):\n",
    "        model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, length):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [idx_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1.\n",
    "        ix = np.argmax(model.predict(X[:, :i+1,:])[0], 1)\n",
    "        y_char.append(idx_to_char[ix[-1]])\n",
    "    return ''.join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "# callback to save the model if better\n",
    "filepath=\"tgt_model.hdf5\"\n",
    "save_model_cb = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "# callback to stop the training if no improvement\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# callback to generate text at epoch end\n",
    "class generateText(Callback):\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        print(generate_text(self.model, 100))\n",
    "generate_text_cb = generateText()\n",
    "\n",
    "callbacks_list = [save_model_cb]\n",
    "\n",
    "def train(model):\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=2, \n",
    "              epochs=NB_EPOCHS, callbacks=callbacks_list, \n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "    model.save_weights('text_gen_got.hdf5')\n",
    "\n",
    "def load_weigths(model):\n",
    "    model.load_weights('text_gen_got.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = createModel();\n",
    "load_weigths(model)\n",
    "for i in range(5):\n",
    "    print(i,\"\\n\",generate_text(model, GENERATE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model](images/model_description.png)\n",
    "The input shape of the text data is ordered as follows : (batch size, number of time steps, hidden size). In other words, for each batch sample and each word in the number of time steps, there is a 500 length embedding word vector to represent the input word. These embedding vectors will be learnt as part of the overall model learning. The input data is then fed into two “stacked” layers of LSTM cells (of 500 length hidden size) – in the diagram above, the LSTM network is shown as unrolled over all the time steps. The output from these unrolled cells is still (batch size, number of time steps, hidden size).\n",
    "\n",
    "This output data is then passed to a Keras layer called TimeDistributed, which will be explained more fully below. Finally, the output layer has a softmax activation applied to it. This output is compared to the training y data for each batch, and the error and gradient back propagation is performed from there in Keras. The training y data in this case is the input x words advanced one time step – in other words, at each time step the model is trying to predict the very next word in the sequence. However, it does this at every time step – hence the output layer has the same number of time steps as the input layer. This will be made more clear later.\n",
    "\n",
    "There is a special Keras layer for use in recurrent neural networks called TimeDistributed. This function adds an independent layer for each time step in the recurrent model. So, for instance, if we have 10 time steps in a model, a TimeDistributed layer operating on a Dense layer would produce 10 independent Dense layers, one for each time step. The activation for these dense layers is set to be softmax in the final layer of our Keras LSTM model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
